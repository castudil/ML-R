library(ggplot2)
data(iris)
ggplot(iris, aes(x = Sepal.Length, y = Sepal.Width, color = Species)) +
geom_point() +
labs(title = "Iris Data",
x = "Sepal Length",
y = "Sepal Width",
color = "Species") +
theme_minimal()
# load diabetes dataset
data("diabetes", package = "lars")
library(lars)
require(lars)
install.packages("lars")
#install.packages("lars")
library(lars)
# load diabetes dataset
data("diabetes", package = "lars")
# head diabetes
head(diabetes)
# plot missing values
plot_missing_values(diabetes)
# plot missing values using the DataExplorer package
#install.packages("DataExplorer")
library(DataExplorer)
plot_missing(diabetes)
# head diabetes
head(diabetes)
#print missing values
print(sum(is.na(diabetes)))
dim(diabetes)
# head diabetes
head(diabetes)
dim(diabetes)
# head diabetes
head(diabetes)
diabetes <- read.csv("diabetes.csv", header = TRUE)
diabetes <- read.csv("diabetes.csv", header = TRUE)
dim(diabetes)
# head diabetes
head(diabetes)
#print missing values
print(sum(is.na(diabetes)))
# split the data into training and test sets
set.seed(123)
trainingRowIndex <- sample(1:nrow(diabetes), 0.8*nrow(diabetes))
trainingData <- diabetes[trainingRowIndex,]
testData  <- diabetes[-trainingRowIndex,]
# train knn
library(class)
predictedSpecies <- knn(training = trainingData[,1:8], test = testData[,1:8], cl = trainingData$Outcome, k = 21)
## create a function that find the best K for KNN
findBestK <- function(trainingData, testData, maxK){
accuracy <- rep(0, maxK)
for (i in 1:maxK){
model <- knn(train = trainingData[,-9], test = testData[,-9], cl = trainingData$Outcome, k = i)
cm <- table(testData$Outcome, model)
accuracy[i] <- sum(diag(cm))/sum(cm)
}
return(accuracy)
}
findBestK(trainingData, testData, 9)
predictedSpecies <- knn(train = trainingData[,-9], test = testData[,-9], cl = trainingData$Outcome, k = 3)
# confusion matrix
cm <- table(testData$Outcome, predictedSpecies)
print(cm)
## create a function that find the best K for KNN
findBestK <- function(trainingData, testData, maxK){
accuracy <- rep(0, maxK)
for (i in 1:maxK){
model <- knn(train = trainingData[,-9], test = testData[,-9], cl = trainingData$Outcome, k = i)
cm <- table(testData$Outcome, model)
accuracy[i] <- sum(diag(cm))/sum(cm)
}
return(accuracy, which.max(accuracy),max(accuracy),model,cm)
}
results<-findBestK(trainingData, testData, 9)
## create a function that find the best K for KNN, the function return an object containing accuracy,model,cm
findBestK <- function(trainingData, testData, maxK){
accuracy <- rep(0, maxK)
models <- list()
cms <- list()
for (i in 1:maxK){
model <- knn(train = trainingData[,-9], test = testData[,-9], cl = trainingData$Outcome, k = i)
cm <- table(testData$Outcome, model)
accuracy[i] <- sum(diag(cm))/sum(cm)
models[[i]] <- model
cms[[i]] <- cm
}
bestK <- which.max(accuracy)
return(list(accuracy = accuracy, model = models[[bestK]], cm = cms[[bestK]]))
}
# test the function
bestK <- findBestK(trainingData, testData, 20)
print(bestK$accuracy)
print(bestK$cm)
## create a function that find the best K for KNN, the function return an object containing accuracy,model,cm
findBestK <- function(trainingData, testData, maxK){
accuracy <- rep(0, maxK)
models <- list()
cms <- list()
for (i in 1:maxK){
model <- knn(train = trainingData[,-9], test = testData[,-9], cl = trainingData$Outcome, k = i)
cm <- table(testData$Outcome, model)
accuracy[i] <- sum(diag(cm))/sum(cm)
models[[i]] <- model
cms[[i]] <- cm
}
bestK <- which.max(accuracy)
return(list(accuracy = accuracy, model = models[[bestK]], cm = cms[[bestK]], k))
}
# test the function
bestK <- findBestK(trainingData, testData, 20)
## create a function that find the best K for KNN, the function return an object containing accuracy,model,cm
findBestK <- function(trainingData, testData, maxK){
accuracy <- rep(0, maxK)
models <- list()
cms <- list()
for (i in 1:maxK){
model <- knn(train = trainingData[,-9], test = testData[,-9], cl = trainingData$Outcome, k = i)
cm <- table(testData$Outcome, model)
accuracy[i] <- sum(diag(cm))/sum(cm)
models[[i]] <- model
cms[[i]] <- cm
}
k <- which.max(accuracy) # identify the best K
return(list(accuracy = accuracy, model = models[[k]], cm = cms[[k]], k))
}
# test the function
bestK <- findBestK(trainingData, testData, 20)
# print a message showing the best K
print(paste("The best K is ", bestK$k))
## create a function that find the best K for KNN, the function return an object containing accuracy,model,cm
findBestK <- function(trainingData, testData, maxK){
accuracy <- rep(0, maxK)
models <- list()
cms <- list()
for (i in 1:maxK){
model <- knn(train = trainingData[,-9], test = testData[,-9], cl = trainingData$Outcome, k = i)
cm <- table(testData$Outcome, model)
accuracy[i] <- sum(diag(cm))/sum(cm)
models[[i]] <- model
cms[[i]] <- cm
}
k <- which.max(accuracy) # identify the best K
return(list(accuracy = accuracy, model = models[[k]], cm = cms[[k]], k))
}
# test the function
bestK <- findBestK(trainingData, testData, 20)
# print a message showing the best K
print(paste("The best K is ", bestK$k))
bestK$k
## create a function that find the best K for KNN, the function return an object containing accuracy,model,cm
findBestK <- function(trainingData, testData, maxK){
accuracy <- rep(0, maxK)
models <- list()
cms <- list()
for (i in 1:maxK){
model <- knn(train = trainingData[,-9], test = testData[,-9], cl = trainingData$Outcome, k = i)
cm <- table(testData$Outcome, model)
accuracy[i] <- sum(diag(cm))/sum(cm)
models[[i]] <- model
cms[[i]] <- cm
}
bestK <- which.max(accuracy) # identify the best K
return(list(accuracy = accuracy, model = models[[bestK]], cm = cms[[bestK]], k=bestK))
}
# test the function
bestK <- findBestK(trainingData, testData, 20)
# print a message showing the best K
print(paste("The best K is ", bestK$k))
# print a message showing the best K
print(paste("The best K is ",bestK$k))
print(bestK$accuracy)
print(bestK$cm)
# confusion matrix
cm <- table(testData$Outcome, bestK$model)
print(cm)
# accuracy
accuracy <- sum(diag(cm))/sum(cm)
print(accuracy)
library(caret)
confusionMatrix(cm)
# print most common performance metrics (acc,F1,precision, recall, etc)
print(performance_metrics(cm))
# Librerías necesarias
install.packages(c("class", "caret", "dplyr"))
library(class)
library(caret)
library(dplyr)
# Cargar el dataset Iris
data(iris)
# Explorar las primeras filas del dataset
head(iris)
# División del dataset en entrenamiento y prueba (80% - 20%)
set.seed(123)
train_index <- createDataPartition(iris$Species, p = 0.8, list = FALSE)
train_data <- iris[train_index, ]
test_data <- iris[-train_index, ]
# Separar características y etiquetas
train_X <- train_data[, -ncol(train_data)]
train_Y <- train_data$Species
test_X <- test_data[, -ncol(test_data)]
test_Y <- test_data$Species
# Aplicar KNN con distintos valores de k
k_values <- c(1, 3, 5, 7, 9)
results <- data.frame(k = integer(), Accuracy = double())
for (k in k_values) {
knn_model <- knn(train = train_X, test = test_X, cl = train_Y, k = k)
confusion_mat <- confusionMatrix(as.factor(knn_model), test_Y)
results <- rbind(results, data.frame(k = k, Accuracy = confusion_mat$overall['Accuracy']))
}
# Mostrar resultados de la precisión para cada k
print(results)
# Evaluar con el mejor valor de k
best_k <- results$k[which.max(results$Accuracy)]
knn_best_model <- knn(train = train_X, test = test_X, cl = train_Y, k = best_k)
# Matriz de confusión final
final_conf_matrix <- confusionMatrix(as.factor(knn_best_model), test_Y)
print(final_conf_matrix)
cat("Actividad completada con éxito. ¡Bien hecho!")
best_k
best_k
# Matriz de confusión final
final_conf_matrix <- confusionMatrix(as.factor(knn_best_model), test_Y)
# Matriz de confusión final
final_conf_matrix <- confusionMatrix(as.factor(knn_best_model), test_Y)
print(final_conf_matrix)
